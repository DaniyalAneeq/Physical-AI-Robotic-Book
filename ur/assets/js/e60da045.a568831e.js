"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4],{5972:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>d,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4/vision-grounded-action","title":"Vision-Grounded Action Understanding","description":"This chapter focuses on integrating multimodal perception models for grounded object recognition and contextual understanding.","source":"@site/docs/module4/04-vision-grounded-action.md","sourceDirName":"module4","slug":"/module4/vision-grounded-action","permalink":"/Physical-AI-Robotic-Book/ur/docs/module4/vision-grounded-action","draft":false,"unlisted":false,"editUrl":"https://github.com/DaniyalAneeq/Physical-AI-Robotic-Book/tree/main/AIdd-book/docs/module4/04-vision-grounded-action.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Vision-Grounded Action Understanding"},"sidebar":"bookSidebar","previous":{"title":"LLM-based Cognitive Planning for Robotics","permalink":"/Physical-AI-Robotic-Book/ur/docs/module4/llm-cognitive-planning"},"next":{"title":"Integrated VLA Humanoid Pipeline","permalink":"/Physical-AI-Robotic-Book/ur/docs/module4/integrated-vla-humanoid-pipeline"}}');var t=i(4848),s=i(8453);const r={title:"Vision-Grounded Action Understanding"},d="Chapter 4: Vision-Grounded Action Understanding",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Practical Hands-on Tasks",id:"practical-hands-on-tasks",level:2}];function l(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-4-vision-grounded-action-understanding",children:"Chapter 4: Vision-Grounded Action Understanding"})}),"\n",(0,t.jsx)(e.p,{children:"This chapter focuses on integrating multimodal perception models for grounded object recognition and contextual understanding."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate multimodal perception models for grounded object recognition."}),"\n",(0,t.jsx)(e.li,{children:"Enable robots to act based on visual information."}),"\n",(0,t.jsx)(e.li,{children:'Understand the concept of "grounding" language to visual entities.'}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multimodal LLMs"}),": Using LLMs that can process both text and images."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision Transformers (ViT)"}),": A brief introduction to Vision Transformers."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Detection APIs"}),": Using pre-trained models for object detection."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Semantic Mapping"}),": Building a map of the environment with semantic information."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"practical-hands-on-tasks",children:"Practical Hands-on Tasks"}),"\n",(0,t.jsx)(e.p,{children:"We will use a simple vision model to identify objects in a simulated environment and integrate this visual feedback into the LLM planning loop."})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(l,{...n})}):l(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>d});var o=i(6540);const t={},s=o.createContext(t);function r(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function d(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);