"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[519],{3928:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"module4/whisper-voice-to-action","title":"Whisper Voice-to-Action Pipeline","description":"In this chapter, we will build a voice command recognition system using OpenAI\'s Whisper model and integrate it into a ROS 2 framework.","source":"@site/docs/module4/02-whisper-voice-to-action.md","sourceDirName":"module4","slug":"/module4/whisper-voice-to-action","permalink":"/Physical-AI-Robotic-Book/docs/module4/whisper-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/DaniyalAneeq/Physical-AI-Robotic-Book/tree/main/AIdd-book/docs/module4/02-whisper-voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Whisper Voice-to-Action Pipeline"},"sidebar":"bookSidebar","previous":{"title":"Introduction to Vision-Language-Action (VLA)","permalink":"/Physical-AI-Robotic-Book/docs/module4/introduction-to-vla"},"next":{"title":"LLM-based Cognitive Planning for Robotics","permalink":"/Physical-AI-Robotic-Book/docs/module4/llm-cognitive-planning"}}');var o=n(4848),s=n(8453);const c={title:"Whisper Voice-to-Action Pipeline"},r="Chapter 2: Whisper Voice-to-Action Pipeline",l={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Practical Hands-on Tasks",id:"practical-hands-on-tasks",level:2}];function d(e){const i={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.header,{children:(0,o.jsx)(i.h1,{id:"chapter-2-whisper-voice-to-action-pipeline",children:"Chapter 2: Whisper Voice-to-Action Pipeline"})}),"\n",(0,o.jsx)(i.p,{children:"In this chapter, we will build a voice command recognition system using OpenAI's Whisper model and integrate it into a ROS 2 framework."}),"\n",(0,o.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Implement a voice command recognition system using the Whisper API."}),"\n",(0,o.jsx)(i.li,{children:"Integrate the voice command system with ROS 2."}),"\n",(0,o.jsx)(i.li,{children:"Parse voice commands to trigger basic robotic actions."}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"OpenAI Whisper API"}),": We will learn how to use the Whisper API for speech-to-text processing."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsxs)(i.strong,{children:["ROS 2 ",(0,o.jsx)(i.code,{children:"rclpy"})]}),": We will use the ROS 2 Python client library to create nodes, publishers, and subscribers."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Voice Command Parsing"}),": Techniques for extracting meaningful commands from transcribed text."]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"practical-hands-on-tasks",children:"Practical Hands-on Tasks"}),"\n",(0,o.jsx)(i.p,{children:"We will create a ROS 2 node that listens for voice commands, transcribes them using Whisper, and publishes the resulting text to a ROS 2 topic."})]})}function h(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>c,x:()=>r});var t=n(6540);const o={},s=t.createContext(o);function c(e){const i=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:c(e.components),t.createElement(s.Provider,{value:i},e.children)}}}]);